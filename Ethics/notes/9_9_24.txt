
Reading notes:

Awad et al. 2018
global preferences
  - prefer sparing humans over animals
  - prefer to spare more lives
  - prefer to spare younger lives
- German ethical guidelines for automated and connected driving
  - states that human life should be preserved over other animal life in dilemmas
  - ambiguous on question of sacrificing the few to save the many
  - very clear about not discriminately saving lives based on personal features such as age
individual variations
- not greatly significant, other than stronger affinity towards a decision vs more variation
culutural clusters
- Southern cluster much more likely to spare young rather than old, eastern cluster opposite
  - same for sparing higher status characters
- southern cluster has much weaker preference for sparing humans over pets than other two clusters
- all in agreement on sparing pedestrians (weak) and the lawful (moderate)
- southern cluster strong preference for sparing women and fit people
country level predictors
- countries closer in culture play the Moral Machine game more similarly
- individualistic cultures, emphasizing the distinct value of each individual, prefer sparing the greater number of characters
- collectivistic cultures, which emphasize respecting elders in a community, have a weaker preference for sparing younger people
- economic prosperity and quality of rules and institution correlate with a stronger preference against sparing the illegally crossing pedestrian
- countries with higher economic inequality correlate with a greater difference in the treatment of characters of different social status
- differential treatment of male and female characters in the MM correspond to country level gender gap in health and survival

Ryan Held & Yang 2024
- fine tuning phase of LLM training in which developers have a much higher degree of control over the behavior and character of a chat bot
  - who will give feedback
  - what prompts/tasks are in domain
  - who will provide exemplar responses
  - i.e. whose preferences are we aligning LLMs with and whose preferences are we missing?
global representation
- alignment improves performance in all dialects but increases disparity between dialects
  - whenever changes to performance are significant (p<0.05), alignment steps increase US English performance much mroe significantly than other global English dialects
  - before alignment, all base models perform about the same across dialects
  - Supervised Fine Tuning improves performance across the board, bu for Indian English the least, Nigerian English second, and American English the most
  - Preference Training shows weaker differences, but still suggests an improvement to the disparity between US English and other dialects
global representation: languages
- alignment for English can improve multilingual performance
  - for reading comprehension, alignment for English produces significant improvements across most languages, and no significant decreases in performance
  - for TyDiQA extractive QA task, Tulu and Starling improved in most languages
    - Zephyr performance decreases singificantly in 6/9 languages, all models worsen in Bengali
- multilinguality in Tulu SFT data explains improvement in multilingual QA performance
global representation: opinions
- alignment increases relative agreement with the USA vs Jordan, China, and Nigeria
  - for all models, initially agree with Nigeria more before alignment, and after alignment agree more with USA
- alignment does not increase relative agreement with USA vs western nations like Germany or Australia
- LLMs align wiht western preferences, exacerbated by alignment
reward model probing
- Starling RM learns to prefer US opinions
- reward models have little influence on out-of-distribution preferences
  - pre-training data defines model behavior on out-of-distribution preferences
  - if opinionated country preferences don't show up in preference-tuning, reward signal doesn't steer the LLM and it retains the preferences of the base model
discussion and conclusions
- alignment of language models is not a one-size-fits-all solution
  - each step of alignment adds additional complexities and impacts on end users
  - transparency of the entire alignment process is of high importance
- slightly multilingual SFT data can have an outsized impact
  - any languages can benefit from multilingual data
  - 13.1% of Tulu dataset is in any language other than English, yet this multilingual data leads to performance improvements for 6/9 tested languages for extractive QA,
    and all 9 languages for reading comprehension
  - this improvement is not a trade-off - English still improves the most on reading comprehension with multilingual data
- reward models do not shape model preferences on out-of-distribution settings
  - beyond the reward model itself, selection of the original SFT data and PT prompts significantly shape the possible impacts of PT


lecture notes:

distinction between ethical AI vs AI ethics

AI ethics/roboethics
- ethics we've identified for us
- our values
- biases
- data privacy
- identify for us, integrate within the system

ethical AI/machine ethics
- if/when these system acquire a human-like level of consciousness/agency, what will their values be
- they could have human-like ethics
  - using human data, should have human values
- they could be separate machine ethics, possibly in contradiction to human ethics
  - could interpret our data differently than the human brain
- legal system
  - how does it apply to machines
  - will it be the same or contradict ours

start with AI ethics (address ethical AI later)

why is it important
- current issues
  - results of generative AI
    - deepfakes
      - politics - attribute statements to political figures, fake news/disinformation
      - pornography - no current laws to deal with criminal pornographic material that is deepfaked (CP)
  - bias/discrimination
    - healthcare
    - FRT - facial recognition technology
    - finance, banking, (insurance?)
  - human rights violations
  - job displacement/replacement
    - autonomous systems taking over human tasks
    - do we need measures to make different types of jobs
    - universal basic income?
  - life vs death situations

levels of autonomy
- autonomy
  - acting on your own
  - control
  - decision making
  - agency
  - partial vs complete
    - where does accountability lie?
    - beneficial
      - human machine interaction

universal declaration of human rights
- not legislation, but a charter
- inspires human rights laws around the world
- article 2: protected classes

2017 asilomar AI conference
- safety
  - should be safe and secure throughout lifecycle
- failure transparency
- judicial transparency
- responsibility
  - designers are stakeholders 
etc.

2016 GDPR (general data protection regulation) - data privacy, xAI
- california consumer protection act modeled after GDPR


AI ethics principles to address
1. safety and reliability
2. transparency (xAI)
3. privacy and security
4. bias, discrimination
5. responsibility, accountability
6. environment, sustainability
7. enablement of AI access

2019
>= 15 docs related to AI guidelines/principles created in US,EU
~5 for UK, Japan
0 for rest of world

1. West (US + EU) making technology
2. human values - US
  - alignment with humanity?
3. latin america, carribean, african nations, asia, china, middle east...


2021
US data centers focused partly in mid-atlantic, but mainly silicon valley
1. West -> US
2. everything developed with US interests in mind
3. kinda ignore everyone else

silicon valley has a lot of shell companies
- hire global south workers to hand label data for cheap
ACM FAT conference

industry
- in 1950s, government funded academia was developing AI
- now, shifted to industry
- concerned more with profit than technological development for the sake of knowledge

moral machine paper

trolley problem
1. more vs fewer
2. agency/active vs passive/fate
3. agency --> responsibility
4. priorities


