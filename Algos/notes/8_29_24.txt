Today: Asymptotics, Recurrences

Asymptotics:
- Intuitively: hide constants and lower order terms since we only care about what happens "at scale" (asymptotically)

Big notation
- use big-oh, big-omega, bit-theta
- big-oh
  - g(n) in O(f(n)) if there exist constants c, n0 > 0 s.t. g(n) <= cf(n) for all n > n0
  - when is g(n) in O(f(n))?
    - when there is an n0 s.t. for any n > n0, f(n) can be scaled by a factor c to be at least g(n)
  - trig functions
    - some functions don't have limits
    - therefore, big-oh is not concerned with limits, but rather with bounding functions
  - Technically: O(f(n)) is a set
  - Abuse notation: g(n) is O(f(n)) or g(n) = O(f(n))
  e.g. 2n^2 + 27 = O(n^2): set n0 = 6 and c = 3
  e.g. 2n^2 + 27 = O(n^3): same values, or n0 = 4 and c = 1
  e.g. n^3 + 2000n^2 + 2000n = O(n^3): set n0 = 10000 and c = 2

thm: 2n^2 + 27 = O(n^2)
  set c = 3. suppose 2n^2 + 27 > cn^2 = 3n^2
  => n^2 < 27 => n < 6
  => 2n^2 + 27 <= 3n^2 for all n >= 6
  set n0 = 6. then 2n^2 + 27 <= cn^2 for all n > n0. QED

- big-omega
  - g(n) in big-theta(f(n)) if there exists constants c,n0 > 0 s.t. g(n) >= cf(n) for all n > n0
  - when is g(n) in big-omega(f(n))?
    - when there is an n0 s.t. for any n > n0, f(n) can be scaled by a factor c to be at most g(n)
  e.g. 2n^2 + 27 = big-omega(n^2): set n_0 = 1 and c = 1
  e.g. 2n^2 + 27 = big-omega(n): set n_0 = 1 and c = 1
  e.g. \frac{1}{100}n^3

- big-theta
  - g(n) in big-theta(f(n)) is g(n) in O(f(n)) AND g(n) in big-omega(f(n))
  - note: constants n_0, c can be different in the pfs for O(f(n)) and big-omega(f(n))
  - equivalent def: g(n) in big-theta(f(n)) if there are constants c_1, c_2, n_0 > 0 s.t. c_1f(n) <= g(n) <= c_2f(n) for all n > n_0
  - both lower and upper bound, so asymptotic equality

Little notation
- strict versions of big-oh and big-omega
- little-oh
  - g(n) in o(f(n)) if for every constant c > 0 there exists a constant n_0 > 0 s.t. g(n) < cf(n) for all n > n_0
- little-omega
  - g(n) in little-omega(f(n)) if for every constant c > 0 there exists a constant n_0 > 0 s.t. g(n) > cf(n) for all n > n_0

e.g. 2n^{2} + 27 = o(n^{2}logn)
e.g. 2n^{2} + 27 = little-omega(n)

- if g(n) is big-omega(f(n)), then it cannot be o(f(n))
- if g(n) is O(f(n)), then it cannot be little-omega(f(n))
- g(n) = o(f(n)) implies g(n) = O(f(n)), same with omegas


Sorting:
- many algorithms recursive, so running time naturally a recurrence relation (karatsuba, strassen).
sorting
- Let T(n) denote (worst-case) running time on an array of size n
  - Selection Sort
    - find smallest unsorted element, put it just after sorted elements. repeat.
    - running time: takes O(n) time to find smallest unsorted element, decreases remaining unsorted elements by 1
    ==> T(n) = T(n - 1) + cn
    - T(n - 1) is the recursive call
    - cn is the time it takes for the current level
  - Mergesort
    - split array into left and right halves, recursively sort each half, then merge.
    - running time: merging takes O(n) time. two recursive calls on half the size.
    ==> T(n) = T(\frac{n}{2}) + T(\frac{n}{2}) + cn = 2T(\frac{n}{2}) + cn
- Also need a base case. For algorithms, constant size input takes constant time.
==> T(n) <= c for all n <= n_0 for some constants n_0, c > 0


Solving Recurrences:

Guess and Check
- T(n) = 3T(\frac{n}{3}) + n    T(1) = 1
- Guess: T(n) <= cn
- Check: assume true for n' < n, prove true for n (strong induction)
  - T(n) = 3T(\frac{n}{3}) + n <= \frac{3cn}{3} + n = (c + 1)n
    - in this case, c scales with n, so c is not a constant, t.f. the pf fails to prove that T(n) <= cn
- Better Guess? what goes up by 1 when n goes up by a factor of 3? log_3(n)
- Guess: T(n) <= nlog_3(3n)
- Check: assume true for n' < n, prove true for n (strong induction)
  - T(n) = 3T(\frac{n}{3}) + n <= 3(\frac{n}{3})log_3(\frac{3n}{3}) + n = nlog_3(n) + n = n(log_3(n) + log_3(3)) = nlog_3(3n)

e.g. Selection Sort. T(n) = T(n - 1) + cn
- Idea: "unroll" the recurrence
  - T(n) = cn + T(n - 1)
  - = cn + c(n - 1) + T(n - 2)
  - = cn + c(n - 1) + c(n - 2) + T(n - 3)
  ...
  - = cn + c(n - 1) + c(n - 2) + ... + c
- n terms, each of which at most cn ==> T(n) <= cn^{2} = O(n^{2})
- At least \frac{n}{2} terms which are at least \frac{cn}{2} ==> T(n) >= c\frac{n^{2}}{4} = big-omega(n^{2})
==> T(n) = big-theta(n^{2})

Recursion Tree: Mergesort
- Generalizes unrolling: draw out full tree of "recursive calls"
- Mergesort: T(n) = 2T(\frac{n}{2}) + cn
***
(tree in lecture slides)
***
- number of levels: log_2(n)
- contribution of level i: 2^{i - 1}\frac{cn}{2^{i - 1}} + cn?

Recursion Tree: Strassen
- T(n) = 7T(\frac{n}{2}) + cn^{2}
***
(tree in lecture slide 15)
***
- number of levels: 
- contribution of level i: 7^{i - 1}c(\frac{n}{2^{i - 1}})^{2} = (\frac{7}{4})^{i - 1}cn^{2}
- Total: cn^{2} \SUM{i=1}{log_2(n+1)}(\frac{7}{4}^{i-1})
==> T(n) = ... = O(n^{log_2(7)})

Master Theorem
- T(n) = aT(\frac{n}{b}) + cn^{k}       T(1) = c
- a,b,c,k constants with a >= 1, b > 1, c > 0, 
***
(see lecture slide 17,18)
***

***NOTE ON LOGARITHMIC FUNCTIONS***
- log_2 takes an input of 2 and returns an output of 1
- log_3 takes an input of 3 and returns an output of 1
- in general, logarithmic functions take an input of their base, and return an output of 1
  - OR, takes the order of magnitude of their base, and returns the exponent
  - e.g. log_2(2^4) = 4












